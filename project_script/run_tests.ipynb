{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc07ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d77ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_column(path,column_list=(0), has_header=True):\n",
    "    \"\"\"Load numeric CSV with missing values -> np.nan using only numpy.\n",
    "    path: path to CSV file\n",
    "    column_list: list of column indices to load (default: 0)\n",
    "    has_header: whether the CSV file has a header row (default: True)\n",
    "    \"\"\"\n",
    "    skip = 1 if has_header else 0\n",
    "    data = np.genfromtxt(\n",
    "        path,\n",
    "        delimiter=\",\",\n",
    "        skip_header=skip,\n",
    "        usecols=(column_list),  # adjust based on relevant columns]       # force float to accommodate np.nan\n",
    "        missing_values=0,       # treat empty strings as missing\n",
    "        filling_values=0,   # replace missing with np.nan\n",
    "        autostrip=True,\n",
    "        invalid_raise=False\n",
    "    )\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "936040bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_columns_name(path):\n",
    "    \"\"\"Load column names from CSV header.\"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        header = f.readline().strip()\n",
    "    return header.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c39477aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deprecated\n",
    "def get_column_index(path, column_name):\n",
    "    \"\"\"Get the index of a column given its name.\"\"\"\n",
    "    columns = load_columns_name(path)\n",
    "    try:\n",
    "        return columns.index(column_name)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a09dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_index_from_list(col_list,col_names):\n",
    "    \"\"\"Get the index of a column given its name from a list of column names.\n",
    "    col_list: list of column names to find\n",
    "    col_names: list of all column names\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return [col_names.index(name) for name in col_list]\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"One of the columns '{col_list}' not found in the provided list.\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b2438c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_column(col,col_name):\n",
    "    '''\n",
    "    Expand a 1D column vector into a N array where N is the number of unique values in col.\n",
    "    Each column in the output array is a binary indicator (0 or 1) of whether the corresponding\n",
    "    entry in col matches the unique value for that column.\n",
    "    col: 1D numpy array of categorical values\n",
    "    '''\n",
    "    unique_values = np.unique(col)\n",
    "    print(f\"Unique values in column '{col_name}': {unique_values}\")\n",
    "    expanded = np.zeros((col.size, unique_values.size), dtype=int)\n",
    "    for i, val in enumerate(unique_values):\n",
    "        expanded[:, i] = (col == val).astype(int)\n",
    "    col_names = [f\"{col_name}_{val}\" for val in unique_values]\n",
    "    return expanded, col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16bbc261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '1' '3' '2' '1' '']\n",
      "[[0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]]\n",
      "['category_', 'category_1', 'category_2', 'category_3']\n"
     ]
    }
   ],
   "source": [
    "col=np.array([1,2,1,3,2,1,\"\"])\n",
    "expanded_col, col_names = expand_column(col, \"category\")\n",
    "print(col)\n",
    "print(expanded_col)\n",
    "print(col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2271bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "\n",
    "x_test = load_data('../data/dataset/x_test.csv')\n",
    "x_train = load_data('../data/dataset/x_train.csv')\n",
    "y_train = load_data('../data/dataset/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f4f21a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53. 33. 20. ... 39. 33. 32.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fae79879",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col_name = load_columns_name('../data/dataset/x_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01cc76cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', '_STATE', 'FMONTH', 'IDATE', 'IMONTH', 'IDAY', 'IYEAR', 'DISPCODE', 'SEQNO', '_PSU', 'CTELENUM', 'PVTRESD1', 'COLGHOUS', 'STATERES', 'CELLFON3', 'LADULT', 'NUMADULT', 'NUMMEN', 'NUMWOMEN', 'CTELNUM1', 'CELLFON2', 'CADULT', 'PVTRESD2', 'CCLGHOUS', 'CSTATE', 'LANDLINE', 'HHADULT', 'GENHLTH', 'PHYSHLTH', 'MENTHLTH', 'POORHLTH', 'HLTHPLN1', 'PERSDOC2', 'MEDCOST', 'CHECKUP1', 'BPHIGH4', 'BPMEDS', 'BLOODCHO', 'CHOLCHK', 'TOLDHI2', 'CVDSTRK3', 'ASTHMA3', 'ASTHNOW', 'CHCSCNCR', 'CHCOCNCR', 'CHCCOPD1', 'HAVARTH3', 'ADDEPEV2', 'CHCKIDNY', 'DIABETE3', 'DIABAGE2', 'SEX', 'MARITAL', 'EDUCA', 'RENTHOM1', 'NUMHHOL2', 'NUMPHON2', 'CPDEMO1', 'VETERAN3', 'EMPLOY1', 'CHILDREN', 'INCOME2', 'INTERNET', 'WEIGHT2', 'HEIGHT3', 'PREGNANT', 'QLACTLM2', 'USEEQUIP', 'BLIND', 'DECIDE', 'DIFFWALK', 'DIFFDRES', 'DIFFALON', 'SMOKE100', 'SMOKDAY2', 'STOPSMK2', 'LASTSMK2', 'USENOW3', 'ALCDAY5', 'AVEDRNK2', 'DRNK3GE5', 'MAXDRNKS', 'FRUITJU1', 'FRUIT1', 'FVBEANS', 'FVGREEN', 'FVORANG', 'VEGETAB1', 'EXERANY2', 'EXRACT11', 'EXEROFT1', 'EXERHMM1', 'EXRACT21', 'EXEROFT2', 'EXERHMM2', 'STRENGTH', 'LMTJOIN3', 'ARTHDIS2', 'ARTHSOCL', 'JOINPAIN', 'SEATBELT', 'FLUSHOT6', 'FLSHTMY2', 'IMFVPLAC', 'PNEUVAC3', 'HIVTST6', 'HIVTSTD3', 'WHRTST10', 'PDIABTST', 'PREDIAB1', 'INSULIN', 'BLDSUGAR', 'FEETCHK2', 'DOCTDIAB', 'CHKHEMO3', 'FEETCHK', 'EYEEXAM', 'DIABEYE', 'DIABEDU', 'CAREGIV1', 'CRGVREL1', 'CRGVLNG1', 'CRGVHRS1', 'CRGVPRB1', 'CRGVPERS', 'CRGVHOUS', 'CRGVMST2', 'CRGVEXPT', 'VIDFCLT2', 'VIREDIF3', 'VIPRFVS2', 'VINOCRE2', 'VIEYEXM2', 'VIINSUR2', 'VICTRCT4', 'VIGLUMA2', 'VIMACDG2', 'CIMEMLOS', 'CDHOUSE', 'CDASSIST', 'CDHELP', 'CDSOCIAL', 'CDDISCUS', 'WTCHSALT', 'LONGWTCH', 'DRADVISE', 'ASTHMAGE', 'ASATTACK', 'ASERVIST', 'ASDRVIST', 'ASRCHKUP', 'ASACTLIM', 'ASYMPTOM', 'ASNOSLEP', 'ASTHMED3', 'ASINHALR', 'HAREHAB1', 'STREHAB1', 'CVDASPRN', 'ASPUNSAF', 'RLIVPAIN', 'RDUCHART', 'RDUCSTRK', 'ARTTODAY', 'ARTHWGT', 'ARTHEXER', 'ARTHEDU', 'TETANUS', 'HPVADVC2', 'HPVADSHT', 'SHINGLE2', 'HADMAM', 'HOWLONG', 'HADPAP2', 'LASTPAP2', 'HPVTEST', 'HPLSTTST', 'HADHYST2', 'PROFEXAM', 'LENGEXAM', 'BLDSTOOL', 'LSTBLDS3', 'HADSIGM3', 'HADSGCO1', 'LASTSIG3', 'PCPSAAD2', 'PCPSADI1', 'PCPSARE1', 'PSATEST1', 'PSATIME', 'PCPSARS1', 'PCPSADE1', 'PCDMDECN', 'SCNTMNY1', 'SCNTMEL1', 'SCNTPAID', 'SCNTWRK1', 'SCNTLPAD', 'SCNTLWK1', 'SXORIENT', 'TRNSGNDR', 'RCSGENDR', 'RCSRLTN2', 'CASTHDX2', 'CASTHNO2', 'EMTSUPRT', 'LSATISFY', 'ADPLEASR', 'ADDOWN', 'ADSLEEP', 'ADENERGY', 'ADEAT1', 'ADFAIL', 'ADTHINK', 'ADMOVE', 'MISTMNT', 'ADANXEV', 'QSTVER', 'QSTLANG', 'MSCODE', '_STSTR', '_STRWT', '_RAWRAKE', '_WT2RAKE', '_CHISPNC', '_CRACE1', '_CPRACE', '_CLLCPWT', '_DUALUSE', '_DUALCOR', '_LLCPWT', '_RFHLTH', '_HCVU651', '_RFHYPE5', '_CHOLCHK', '_RFCHOL', '_LTASTH1', '_CASTHM1', '_ASTHMS1', '_DRDXAR1', '_PRACE1', '_MRACE1', '_HISPANC', '_RACE', '_RACEG21', '_RACEGR3', '_RACE_G1', '_AGEG5YR', '_AGE65YR', '_AGE80', '_AGE_G', 'HTIN4', 'HTM4', 'WTKG3', '_BMI5', '_BMI5CAT', '_RFBMI5', '_CHLDCNT', '_EDUCAG', '_INCOMG', '_SMOKER3', '_RFSMOK3', 'DRNKANY5', 'DROCDY3_', '_RFBING5', '_DRNKWEK', '_RFDRHV5', 'FTJUDA1_', 'FRUTDA1_', 'BEANDAY_', 'GRENDAY_', 'ORNGDAY_', 'VEGEDA1_', '_MISFRTN', '_MISVEGN', '_FRTRESP', '_VEGRESP', '_FRUTSUM', '_VEGESUM', '_FRTLT1', '_VEGLT1', '_FRT16', '_VEG23', '_FRUITEX', '_VEGETEX', '_TOTINDA', 'METVL11_', 'METVL21_', 'MAXVO2_', 'FC60_', 'ACTIN11_', 'ACTIN21_', 'PADUR1_', 'PADUR2_', 'PAFREQ1_', 'PAFREQ2_', '_MINAC11', '_MINAC21', 'STRFREQ_', 'PAMISS1_', 'PAMIN11_', 'PAMIN21_', 'PA1MIN_', 'PAVIG11_', 'PAVIG21_', 'PA1VIGM_', '_PACAT1', '_PAINDX1', '_PA150R2', '_PA300R2', '_PA30021', '_PASTRNG', '_PAREC1', '_PASTAE1', '_LMTACT1', '_LMTWRK1', '_LMTSCL1', '_RFSEAT2', '_RFSEAT3', '_FLSHOT6', '_PNEUMO2', '_AIDTST3']\n"
     ]
    }
   ],
   "source": [
    "print(x_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb682416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[306, 315]\n",
      "[[2. 3.]\n",
      " [9. 3.]\n",
      " [3. 2.]\n",
      " ...\n",
      " [3. 3.]\n",
      " [3. 3.]\n",
      " [9. 2.]]\n",
      "Unique values in column '_PACAT1': [1. 2. 3. 4. 9.]\n",
      "Unique values in column '_LMTWRK1': [0. 1. 2. 3. 9.]\n",
      "['_PACAT1_1.0', '_PACAT1_2.0', '_PACAT1_3.0', '_PACAT1_4.0', '_PACAT1_9.0', '_LMTWRK1_0.0', '_LMTWRK1_1.0', '_LMTWRK1_2.0', '_LMTWRK1_3.0', '_LMTWRK1_9.0']\n",
      "[[0 1 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 1 ... 1 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 1 0]\n",
      " [0 0 1 ... 0 1 0]\n",
      " [0 0 0 ... 1 0 0]]\n",
      "x_train_expanded saved to x_train_expanded.csv\n"
     ]
    }
   ],
   "source": [
    "col_list = ['_PACAT1','_LMTWRK1']\n",
    "col_indices = get_column_index_from_list(col_list, x_col_name)\n",
    "print(col_indices)\n",
    "#load x_train with only the columns in col_indices\n",
    "x_train_subset = load_data_column('../data/dataset/x_train.csv', column_list=col_indices)\n",
    "print(x_train_subset)\n",
    "#expand the colomns in x train_subset\n",
    "expanded_cols = []\n",
    "expanded_col_names = []\n",
    "for i, col_name in enumerate(col_list):\n",
    "    expanded_col, col_names = expand_column(x_train_subset[:, i], col_name)\n",
    "    expanded_cols.append(expanded_col)\n",
    "    expanded_col_names.extend(col_names)\n",
    "x_train_expanded = np.hstack(expanded_cols)\n",
    "#add the columns name back to x_train_expanded\n",
    "print(expanded_col_names)\n",
    "print(x_train_expanded)\n",
    "x_train_expanded_dataset = np.vstack((expanded_col_names, x_train_expanded))\n",
    "#save x_train_expanded to csv\n",
    "np.savetxt(\"x_train_expanded.csv\", x_train_expanded_dataset, fmt=\"%s\", delimiter=\",\")\n",
    "print(\"x_train_expanded saved to x_train_expanded.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49b28675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "\n",
    "    Args:\n",
    "        y:      shape=(N,)\n",
    "        k_fold: K in K-fold, i.e. the fold num\n",
    "        seed:   the random seed\n",
    "\n",
    "    Returns:\n",
    "        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "\n",
    "    >>> build_k_indices(np.array([1., 2., 3., 4.]), 2, 1)\n",
    "    array([[3, 2],\n",
    "           [0, 1]])\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval : (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(object=k_indices, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "570c88be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load x_train and y_train\n",
    "x_train = load_data_column('x_train_expanded.csv', column_list=None)\n",
    "y_train = load_data_column('../data/dataset/y_train.csv', column_list=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db763246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5034 271803 120350 ... 235713 288612  41205]\n",
      " [115126 279242  14551 ...  91185 220011 312741]\n",
      " [162468 179536  84131 ... 234610 232051 217560]\n",
      " ...\n",
      " [146820 310758  72754 ... 220201  86341  37943]\n",
      " [ 37130 253130  23474 ... 325326  53369 222516]\n",
      " [268584 140632 188469 ... 293372 229520  21440]]\n",
      "Fold 1\n",
      "x_test shape: (32813, 10), y_test shape: (32813, 2)\n",
      "x_train shape: (295322, 10), y_train shape: (295322, 2)\n",
      "Fold 2\n",
      "x_test shape: (32813, 10), y_test shape: (32813, 2)\n",
      "x_train shape: (295322, 10), y_train shape: (295322, 2)\n",
      "Fold 3\n",
      "x_test shape: (32813, 10), y_test shape: (32813, 2)\n",
      "x_train shape: (295322, 10), y_train shape: (295322, 2)\n",
      "Fold 4\n",
      "x_test shape: (32813, 10), y_test shape: (32813, 2)\n",
      "x_train shape: (295322, 10), y_train shape: (295322, 2)\n",
      "Fold 5\n",
      "x_test shape: (32813, 10), y_test shape: (32813, 2)\n",
      "x_train shape: (295322, 10), y_train shape: (295322, 2)\n",
      "Fold 6\n",
      "x_test shape: (32813, 10), y_test shape: (32813, 2)\n",
      "x_train shape: (295322, 10), y_train shape: (295322, 2)\n",
      "Fold 7\n",
      "x_test shape: (32813, 10), y_test shape: (32813, 2)\n",
      "x_train shape: (295322, 10), y_train shape: (295322, 2)\n",
      "Fold 8\n",
      "x_test shape: (32813, 10), y_test shape: (32813, 2)\n",
      "x_train shape: (295322, 10), y_train shape: (295322, 2)\n",
      "Fold 9\n",
      "x_test shape: (32813, 10), y_test shape: (32813, 2)\n",
      "x_train shape: (295322, 10), y_train shape: (295322, 2)\n",
      "Fold 10\n",
      "x_test shape: (32813, 10), y_test shape: (32813, 2)\n",
      "x_train shape: (295322, 10), y_train shape: (295322, 2)\n",
      "x_train shape: (328135, 10), y_train shape: (328135, 2)\n",
      "x_test shape: (32813, 10), y_test shape: (32813, 2)\n"
     ]
    }
   ],
   "source": [
    "#split the dataset in 10 folds and define x_train as 9 out of 10 folds and x_test as the remaining fold, same for y_train and y_test\n",
    "k_fold = 10\n",
    "seed = 1\n",
    "k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "print(k_indices)\n",
    "for k in range(k_fold):\n",
    "    x_test = x_train[k_indices[k]]\n",
    "    y_test = y_train[k_indices[k]]\n",
    "    x_train_fold = np.delete(x_train, k_indices[k], axis=0)\n",
    "    y_train_fold = np.delete(y_train, k_indices[k], axis=0)\n",
    "    print(f\"Fold {k+1}\")\n",
    "    print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")\n",
    "    print(f\"x_train shape: {x_train_fold.shape}, y_train shape: {y_train_fold.shape}\")\n",
    "#print shape of x_train and y_train and x_test and y_test\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dcd2ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "\n",
    "    Returns:\n",
    "        a non-negative loss (scalar)\n",
    "    \"\"\"\n",
    "\n",
    "    # compute the loss: negative log likelihood\n",
    "    y_hat = sigmoid(tx @ w)\n",
    "    loss = -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Logistic regression using gradient descent.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape (N, 1)\n",
    "        tx: numpy array of shape (N, D)\n",
    "        initial_w: numpy array of shape (D, 1)\n",
    "        max_iters: scalar\n",
    "        gamma: scalar\n",
    "\n",
    "    Returns:\n",
    "        losses: list of loss values\n",
    "        ws: list of weights\n",
    "    \"\"\"\n",
    "    ws = [initial_w]\n",
    "    w = initial_w\n",
    "    losses = [calculate_loss(y, tx, w)]\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = calculate_gradient(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "        loss = calculate_loss(y, tx, w)\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return ws[-1], np.asarray(losses[-1])\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\n",
    "\n",
    "    Args:\n",
    "        t: scalar or numpy array\n",
    "\n",
    "    Returns:\n",
    "        scalar or numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    return 1 / (1 + np.exp(-t))\n",
    "\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "\n",
    "    Returns:\n",
    "        a vector of shape (D, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    y_hat = sigmoid(tx @ w)\n",
    "    gradient = tx.T @ (y_hat - y) / y.shape[0]\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss and gradient.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "        lambda_: scalar\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar number\n",
    "        gradient: shape=(D, 1)\n",
    "    \"\"\"\n",
    "    gradient = calculate_gradient(y, tx, w) + lambda_ * 2 * w\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "\n",
    "    return float(loss), gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e141d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: [[-4.86567855e+04  3.97526933e-01]\n",
      " [-2.65778995e+04  2.21741052e-01]\n",
      " [-2.66269771e+04  2.21107166e-01]\n",
      " [-4.15035332e+04  3.15589620e-01]\n",
      " [-2.07013047e+04  1.67431088e-01]\n",
      " [-9.33524484e+02  7.30339647e-03]\n",
      " [-1.56235550e+04  1.07167782e-01]\n",
      " [-3.34106005e+04  2.49287641e-01]\n",
      " [-1.08371796e+05  9.18975422e-01]\n",
      " [-5.72702441e+03  4.06616179e-02]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinal loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#cross validate with x-test and y-test\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m y_pred = sigmoid(\u001b[43mx_test\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m) >= \u001b[32m0.5\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_pred.flatten()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrue labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_test.flatten()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "#run penalized logistic regression on x_train and y_train\n",
    "initial_w = np.zeros((x_train.shape[1], 1))\n",
    "max_iters = 100\n",
    "gamma = 0.01\n",
    "lambda_ = 0.1\n",
    "loss,w = penalized_logistic_regression(y_train, x_train, initial_w, lambda_)\n",
    "print(f\"Final loss: {loss}\")\n",
    "#cross validate with x-test and y-test\n",
    "y_pred = sigmoid(x_test @ w) >= 0.5\n",
    "print(f\"Predictions: {y_pred.flatten()}\")\n",
    "print(f\"True labels: {y_test.flatten()}\")\n",
    "#compute accuracy\n",
    "accuracy = np.mean(y_pred.flatten() == y_test.flatten())\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
